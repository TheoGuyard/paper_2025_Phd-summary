%!TEX aux_directory = aux
%!TEX output_directory = aux
\documentclass[11pt]{article}

% Style file options
%   - []: enable editions macros
%   - [final]: remove edition macros
\usepackage[]{TGstyle}

% ------ Metadata ------ %

\title{Summary of: Branch-and-Bound Algorithms for L0-Regularized Problems}
\author{Théo Guyard \\ Inria, Centre de l'Université de Rennes \\ Insa Rennes, IRMAR -- CNRS UMR 6625 \\ \texttt{theo.guyard@insa-rennes.fr}}
\date{\today}

% ------ Headings ------ %

\input{headings/packages}
\input{headings/macros}
\input{headings/glossaries}
\usepackage{kmath}

% ------ Edition macros ------ %

\ProvideEditionMacros{TG}{blue}

% ------ Document ------ %

\begin{document} 

\maketitle

The manuscript focuses on $\ell_0$-regularized optimization problems given by
\begin{equation}
    \label{prob:prob}
    \tag{$\ppb$}
    \textstyle
    \opt{\pobj} = \min_{\pv \in \kR^{\pdim}} \lfunc(\pv) + \underbrace{\reg\norm{\pv}{0} + \pfunc(\pv)}_{\rfunc(\pv)}
\end{equation}
where $\kfuncdef{\lfunc}{\kR^{\pdim}}{\kR \cup \{+\infty\}}$ is a loss function and $\kfuncdef{\rfunc}{\kR^{\pdim}}{\kR \cup \{+\infty\}}$ is a regularization involving an $\ell_0$-norm with weight $\reg > 0$ to promote sparse solutions by counting the number of non-zeros in its argument, as well as another penalty term $\kfuncdef{\pfunc}{\kR^{\pdim}}{\kR \cup \{+\infty\}}$ used to promote other properties than sparsity.
Although various heuristics have been proposed to tackle this problem \cite{tropp2010computational}, exact methods are of paramount interest for many applications in signal processing, machine learning, or statistics, among other fields \citep{tillmann2021cardinality,bertsimas2016best}.
In this vein, state-of-the-art solvers are based on \gls{bnb} algorithms \citep{ben2022global,hazimeh2022sparse}.
So far, they have only been designed for some particular instances of the problem and can still suffer from poor numerical performance on real-world data.
In this view, the manuscript aims to:
\begin{enumerate}[nosep, label=\arabic*.]
    \item Provide a unified \gls{bnb} algorithm to tackle problem \eqref{prob:prob} in a generic fashion,
    \item Propose novel acceleration strategies to improve its numerical efficiency,
    \item Design a convenient toolbox for practitioners.
\end{enumerate} 
From \Cref{sec:bnb,sec:contributions,sec:numerics}, we describe the main ingredients required to implement a \gls{bnb} algorithm tailored to problem \eqref{prob:prob}, outline the contributions presented in the manuscript, and give an overview of the implications stemming from this work for signal processing applications.


\section{Branch-and-Bound Ingredients}
\label{sec:bnb}

\gls{bnb} is a generic algorithm to address optimization problems.
It explores \emph{regions} in the feasible space and \emph{prunes} those that cannot contain solutions.
When specialized to problem \eqref{prob:prob}, the construction of regions is driven by the sparsity of the optimization variable, that is, by the fixing some entries to zero or non-zero.
Stated otherwise, regions are defined as
\begin{equation}
    \label{eq:region}
    \nodesymbol = \left\{
        \pv \in \kR^{\pdim}
        \left|
        \begin{array}{ll}
            \pvi{\idxentry} = 0 &\forall \idxentry \in \setzero \\
            \pvi{\idxentry} \neq 0 &\forall \idxentry \in \setone \\
            \pvi{\idxentry} \in \kR &\forall \idxentry \in \setnone
        \end{array}
        \right.
    \right\}
\end{equation}
from a partition $(\setzero,\setone,\setnone)$ of the index set $\{1,\dots,\pdim\}$.
Starting with a region resulting from the partition $(\emptyset,\emptyset,\{1,\dots,\pdim\})$ which corresponds to the whole feasible space $\kR^{\pdim}$, the \gls{bnb} algorithm iteratively selects a new region to process, performs a \emph{pruning test}, and if no pruning decision can be taken, splits it into two new subregions by fixing a new index to zero or non-zero.
Once the entire feasible space has been explored, solutions to problem \eqref{prob:prob} can be extracted from the regions that have not been pruned during the search.
The \gls{bnb} complexity depends on the total number of regions processed and the cost to evaluate a pruning test for a given region.

\paragraph{Pruning test.}
The central mechanism of the \gls{bnb} algorithm is a pruning test which aims to detect regions that cannot contain any solution to problem \eqref{prob:prob}.
Denoting 
\begin{equation}
    \label{prob:prob-node}
    \tag{$\node{\ppb}$}
    \textstyle
    \node{\pobj} = \min_{\pv \in \nodesymbol} \lfunc(\pv) + \rfunc(\pv)
\end{equation}
the smallest objective value achievable in problem \eqref{prob:prob} over a region $\nodesymbol$, then no solutions are contained in this part of the feasible space if $\node{\pobj} > \opt{\pobj}$ and the region can thus be pruned.
However, this inequality cannot be checked directly since $\opt{\pobj}$ is unknown and recovering $\node{\pobj}$ is still an NP-hard task.
In practice, one rather implements a pruning test expressed as
\begin{equation}
    \label{eq:pruning-test}
    \node{\LB{\pobj}} > \opt{\UB{\pobj}}
\end{equation}
using some lower bound $\node{\LB{\pobj}} \leq \node{\pobj}$ and upper bound $\opt{\UB{\pobj}} \geq \opt{\pobj}$ as surrogates.
Implementing the \gls{bnb} algorithm then amounts to devising strategies to compute these bounds, typically sought \emph{tight} and \emph{tractable} to ensure the numerical efficiency of the method.

\paragraph{Bounds computation.}
Various heuristics-based strategies have been proposed to obtain an upper bound $\opt{\UB{\pobj}} \geq \opt{\pobj}$ of good quality at a reasonable cost.
The crux of the \gls{bnb} algorithm mainly lies in the computation of a suitable lower bound $\node{\LB{\pobj}} \leq \node{\pobj}$ for each region explored.
This can be achieved by solving a \emph{relaxation} of problem \eqref{prob:prob-node}, expressed as
\begin{equation}
    \label{prob:relax-node}
    \tag{$\node{\rpb}$}
    \textstyle
    \node{\robj} = \min_{\pv \in \kR^{\pdim}} \lfunc(\pv) + \node{\rfunc}(\pv)
\end{equation}
for some $\kfuncdef{\node{\rfunc}}{\kR^{\pdim}}{\kR \cup \{+\infty\}}$.
One requires $\node{\rfunc}(\pv) \leq \rfunc(\pv)$ for all $\pv \in \nodesymbol$ so that setting $\node{\LB{\pobj}} = \node{\robj}$ in the pruning test \eqref{eq:pruning-test} leads to a valid \gls{bnb} algorithm.
Moreover, one usually desires $\node{\rfunc}$ convex so that the relaxation \eqref{prob:relax-node} can be addressed efficiently via cutting-edge convex optimization methods.

\paragraph{Limitations.}
\gls{bnb} algorithms proposed so far in the literature suffer from two main limitations.
First, existing strategies to construct the function $\node{\rfunc}$ in the relaxation \eqref{prob:relax-node} are tied to the expression of the function $\pfunc$, thereby restricting the \gls{bnb} algorithm scope of application.
Second, these \gls{bnb} algorithms can still suffer from poor performance on some real-world instances.
Indeed, one relaxation needs to be solved per region explored to evaluate the associated pruning test, which can represent a significant computational burden.


\section{Contributions}
\label{sec:contributions}

The manuscript presents contributions disseminated from Chapters 4 to 7 to address the \gls{bnb} algorithm limitations aforementioned.
In the following, we outline the main results of each chapter.


\paragraph{Unified Branch-and-Bound Framework.}

Chapter 4 proposes a unified \gls{bnb} framework to address \eqref{prob:prob} through a generic strategy for constructing the relaxation \eqref{prob:relax-node}.
Specifically, we consider any function $\pfunc(\pv) = \sum_{\idxentry=1}^{\pdim}\separable{\pfunc}{\idxentry}(\pvi{\idxentry})$ separable into splitting terms $\{\separable{\pfunc}{\idxentry}\}_{\idxentry=1}^{\pdim}$ that are proper, closed, convex, even, super-coercive, and verify $\separable{\pfunc}{\idxentry} \geq \separable{\pfunc}{\idxentry}(0) = 0$ with $0 \in \interior\dom(\separable{\pfunc}{\idxentry})$.\footnote{Our results can be extended to cases where $\separable{\pfunc}{\idxentry}$ is only assumed coercive with $\separable{\pfunc}{\idxentry}(0) \in \kR$.}
In this context, we show that the \emph{convex envelope} of the function $\rfunc$ over a region $\nodesymbol$ defined by \eqref{eq:region} can be expressed as
\begin{equation}
    \node{\rfunc}(\pv) = \sum_{\idxentry=1}^{\pdim}\node{\separable{\rfunc}{\idxentry}}(\pvi{\idxentry})
    \quad\text{with}\quad
    \node{\separable{\rfunc}{\idxentry}}(\pvi{}) = 
    \begin{cases}
        \icvx(\pvi{} = 0) &\text{if } \idxentry \in \setzero \\
        \separable{\pfunc}{\idxentry}(\pvi{}) + \reg &\text{if } \idxentry \in \setone \\
        \separable{\pslope}{\idxentry} \abs{\pvi{}} &\text{if } \idxentry \in \setnone \text{ with }\abs{\pvi{}} \leq \separable{\plimit}{\idxentry} \\
        \separable{\pfunc}{\idxentry}(\pvi{}) + \reg &\text{if } \idxentry \in \setnone \text{ with }\abs{\pvi{}} > \separable{\plimit}{\idxentry}
    \end{cases}
\end{equation}
where $\separable{\pslope}{\idxentry} = \sup\kset{\bvi{} \in \kR}{\conj{\separable{\pfunc}{\idxentry}}(\bvi{}) \leq \reg}$ and $\separable{\plimit}{\idxentry} = \sup\kset{\bvi{} \in \kR}{\bvi{} \in \subdiff\conj{\separable{\pfunc}{\idxentry}}(\pslope)}$.
Here, the notation $\icvx(\cdot)$ stands for the convex indicator function and $\conj{\separable{\pfunc}{\idxentry}}$ for the convex conjugate of $\separable{\pfunc}{\idxentry}$.
Our construction strategy is \emph{optimal} in the sense that it gives the best convex approximation of the function $\rfunc$ over the region $\nodesymbol$.
Notably, it encompasses prior contributions as particular cases, and, in contrast to them, does not depend on a specific expression of the function $\pfunc$.
We additionally characterize various operators associated with the function $\node{\rfunc}$ enabling the use of modern convex optimization methods to address the resulting relaxation \eqref{prob:relax-node}.
From this contribution stems a unified \gls{bnb} framework that can address problem \eqref{prob:prob} generically.

\paragraph{Simultaneous Pruning.}

Chapter 5 introduces a novel acceleration strategy for the \gls{bnb} algorithm.
We first point out that a valid choice for the lower-bound involved in the pruning test \eqref{eq:pruning-test} is to set $\node{\LB{\pobj}} = \node{\dfunc}(\dv)$ for any $\dv \in \kR^{\pdim}$, where $\node{\dfunc}$ denotes the objective of the dual problem associated with the relaxation \eqref{prob:relax-node}.
More importantly, we show that for nested regions $\nodesymbol' \subseteq \nodesymbol$ defined as in \eqref{eq:region}, such \emph{dual} lower-bounds verify
\begin{equation}
    \label{eq:dual-bound-nested}
    \dfunc^{\nodesymbol'}(\dv) = \node{\dfunc}(\dv) + \Delta_{\text{sp}}^{\nodesymbol'}(\dv)
\end{equation}
where $\kfuncdef{\Delta_{\text{sp}}^{\nodesymbol'}}{\kR^{\pdim}}{\kR \cup \{+\infty\}}$ can be evaluated in $\mathcal{O}(1)$.
From a \emph{single} evaluation of $\node{\dfunc}(\dv)$ when processing the region $\nodesymbol$, we can then \emph{simultaneously} check several nested regions $\nodesymbol' \subseteq \nodesymbol$ against a pruning test through their dual bound $\dfunc^{\nodesymbol'}(\dv)$ obtained \emph{at virtually no cost} from \eqref{eq:dual-bound-nested}.
In contrast, the standard pruning strategy requires solving one relaxation per region processed.
This shift of paradigm in the pruning process allows accelerating the \gls{bnb} algorithm up to several orders of magnitude in terms of solving time.

\paragraph{Bound Tightening.}

Chapter 6 focuses on the specific case where $\pfunc(\pv) = 0$ when $\bigL \leq \pv \leq \bigU$ for some $(\bigL,\bigU) \in \kR-^{\pdim} \times \kR+^{\pdim}$, and $\pfunc(\pv) = +\infty$ otherwise.
These instances are of interest for various applications but lead to two conflicting imperatives: the bounds $(\bigL,\bigU)$ are usually required to have large entries in absolute value, but increasing their magnitude degrades the performance of the \gls{bnb} algorithm.
To face this issue, we propose a \emph{bound tightening} strategy to refine the initial bounds $(\bigL, \bigU)$ during the \gls{bnb} algorithm.
Similarly to simultaneous pruning, we show that for a region of the form $\nodesymbol' = \nodesymbol \setminus \kset{\pv \in \kR^{\pdim}}{\bigL' \leq \pv \leq \bigU'}$ with $\bigL \leq \bigL' \leq \bigU' \leq \bigU$, we can perform the pruning test using a dual bound $\node{\LB{\pobj}}=\node{\dfunc}(\dv)$ verifying
\begin{equation}
    \label{eq:dual-bound-peeling}
    \dfunc^{\nodesymbol'}(\dv) = \node{\dfunc}(\dv) + \Delta_{\text{bt}}^{\nodesymbol'}(\dv)
\end{equation}
where $\kfuncdef{\Delta_{\text{bt}}^{\nodesymbol'}}{\kR^{\pdim}}{\kR \cup \{+\infty\}}$ can be evaluated in $\mathcal{O}(1)$.
When processing region $\nodesymbol$, we can then check any nested region $\nodesymbol'$ against a pruning test using its dual bound obtained at virtually no cost via \eqref{eq:dual-bound-peeling}.
If the latter is verified, the region $\nodesymbol'$ can be pruned, which amounts to tightening the bounds associated with region $\nodesymbol$ as $(\bigL,\bigU) \leftarrow (\bigL',\bigU')$.
This process is implemented dynamically in the \gls{bnb} algorithm to refine the initial bounds $(\bigL,\bigU)$, which progressively enhance its performance during the resolution process.

\paragraph{Relaxation Processing.}

Chapter 7 proposes an acceleration strategy for numerical procedures tailored to the relaxation \eqref{prob:relax-node}, but more generally tailored to any \emph{convex} optimization problem of the form
\begin{equation}
    \label{prob:prob-convex}
    \tag{$\rpb$}
    \opt{\robj} = \textstyle\min_{\pv \in \kR^{\pdim}} \lfunc(\pv) + \relaxrfunc(\pv)
\end{equation}
where $\relaxrfunc$ promotes sparse solutions, which are ubiquitous in the literature.
While prior contributions have proposed \emph{screening tests} \cite{el2012safe} to detect the position of zero entries in the problem solutions, we propose \emph{smoothing tests} to detect the position of non-zero entries.
We combine these two complementary strategies to dynamically reduce the dimension and smooth the objective function of problem \eqref{prob:prob-convex} during its resolution process.
When interleaved within standard optimization procedures such as proximal gradient, coordinate descent, or ADMM, this results in performance improvements up to several orders of magnitude in terms of solving time.

\paragraph{Practical Toolbox.}
The contributions presented in the manuscript are implemented in \texttt{El0ps},\footnote{\url{https://github.com/TheoGuyard/El0ps}} an open-source Python package providing convenient interfaces for problem \eqref{prob:prob}.
It includes several built-in instances, but also offers the flexibility to create new ones.
Users are only required to specify some operators associated with the function $\lfunc$ and $\pfunc$.
Once done, the \gls{bnb} solver provided by \texttt{El0ps} can automatically address the resulting instance of problem \eqref{prob:prob}.
In contrast, other existing \gls{bnb} solvers are restricted to one particular instance.
Numerical experiments outline that \texttt{El0ps} achieves state-of-the-art performance, with acceleration factors up to several orders of magnitude in terms of solving times compared to its best competitors, even on instances that were previously thoroughly addressed in the literature.

\section{Numerical Illustration}
\label{sec:numerics}

\Cref{fig:perfprofiles} gives a glimpse of the implications of the manuscript's contributions for signal processing applications.
Here, we consider Bernoulli-Mixture inverse problems \cite{soussen2011bernoulli} aiming to reconstruct a sparse signal $\groundtruth \in \kR^{1000}$ known to verify $\groundtruthi{\idxentry} \sim \mathcal{D}$ with probability $\density = 0.01$ for some distribution $\mathcal{D}$, and $\groundtruthi{\idxentry}=0$ otherwise. 
To this end, we have access to a linear measurement $\obs = \dic\groundtruth + \noise$ where $\dic \in \kR^{500 \times 1000}$ is a given matrix and $\noise \sim \mathcal{N}(\0,\noisestd^2 \mathbf{I})$ is a noise with 10dB signal-to-noise ratio.
In this setup, the Maximum A Posteriori estimation of the sparse signal corresponds to any solution of problem \eqref{prob:prob} with
a loss $\lfunc(\pv) = \frac{1}{2}\norm{\obs - \dic\pv}{2}^2$,
a parameter $\reg = \noisestd^2\log((1-\density)/\density)$,
and a penalty $\pfunc$ whose expression depends on the underlying distribution $\mathcal{D}$ of the non-zero entries.
To conduct our experiment, we select different choices of distributions (Normal, Gauss-Laplace, Half-Normal, Exponential), independently generate 100 instances of the corresponding problem \eqref{prob:prob}, and represent the percentage solved within a given time budget using different numerical procedures.
Compared to generic optimization solvers such as \texttt{Cplex} and \texttt{Mosek} \cite{kronqvist2019review}, but also to solution methods specifically designed for problem \eqref{prob:prob} such as \texttt{L0bnb} \cite{hazimeh2022sparse} and \texttt{OA} \cite{bertsimas2021unified}, our solver \texttt{El0ps} obtains the best performances and can also handle a broader variety of instances. 
For instance, it is the only method able to process cases corresponding to the Exponential distribution.

\begin{figure}[!ht]
    \vspace*{-0.25cm}
    \centering
    \input{imgs/mixtures.tex}
    \vspace*{-0.5cm}
    \caption{Instances solved for a given time budget using: \ref{plot:cplex} \texttt{Cplex}, \ref{plot:mosek} \texttt{Mosek}, \ref{plot:mosek_oa} \texttt{OA}, \ref{plot:l0bnb} \texttt{L0bnb}, \ref{plot:el0ps} \texttt{El0ps}. The absence of a curve indicates that the solver was unable to process the corresponding problem instance.}
    \label{fig:perfprofiles}
    \vspace*{-0.25cm}
\end{figure}


{
    \small
    \bibliography{main}
}


\clearpage

\thispagestyle{empty}

\vspace*{150pt}
\begin{center}
    \LARGE\bf{Lettre de Candidature}
\end{center}
\vspace*{50pt}
\begin{flushright}
    À Rennes, le 16 décembre 2024
\end{flushright}
\vspace*{20pt}
\textbf{Objet:} Candidature au prix de thèse du GRETSI 2025

\vspace*{20pt}

Par cette présente lettre, je souhaiterais soumettre ma candidature au prix de thèse 2025 du GRETSI pour mon manuscrit intitulé \textit{Branch-and-Bound Algorithms for L0-Regularized Optimization Problems} réalisé sous la direction de Cédric Herzet.
Je certifie que les critères de recevabilité de ma candidature sont respectés et m'engage à fournir tous les documents nécessaires à l'examen de mon dossier.
~\\

Veuillez agréer, Madame, Monsieur, l'expression de mes salutations distinguées.

\vspace*{50pt}
\begin{flushright}
    Théo Guyard
\end{flushright}

\end{document}